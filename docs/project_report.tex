\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}

\title{Stateful Cache-Aware Routing for Distributed LLM Inference}
\author{Project Team}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Large Language Model (LLM) inference is memory-bound, with the prefill phase (processing input prompts) being computationally expensive. In distributed serving environments, standard stateless load balancing fails to leverage the locality of Key-Value (KV) caches across requests, leading to redundant recomputation. We propose a \textbf{Stateful Cache-Aware Router} that utilizes a global view of distributed cache states to route requests to workers holding relevant prefixes. Crucially, we introduce a novel \textbf{Asynchronous Eviction Callback} mechanism where vLLM workers actively report cache evictions to the router, ensuring the global state remains consistent without the overhead of distributed shared memory.
\end{abstract}

\section{Introduction}
The efficiency of LLM serving is constrained by the quadratic complexity of the attention mechanism during the prefill phase. While techniques like PagedAttention (vLLM) optimize memory management within a single node, distributed deployments often suffer from "cache blindness." A request with a long system prompt routed to Worker A generates a KV cache that is inaccessible to Worker B. If a subsequent request with the same prompt is routed to Worker B, the cache must be recomputed, wasting GPU cycles and increasing Time-To-First-Token (TTFT).

Existing solutions either rely on complex distributed storage (LMCache) or tightly coupled scheduling (SGLang). Our approach focuses on a lightweight, eventually consistent routing layer that "brings the compute to the data" by tracking where prefixes reside and updating this map via push-based signals from the workers.

\section{Problem Statement}
Given a cluster of $N$ workers $W = \{w_1, ..., w_N\}$ and a stream of requests $R$, our goal is to minimize the total prefill computation time by maximizing the Cache Hit Rate (CHR).
$$ \text{Maximize } \sum_{r \in R} \mathbb{I}(\text{cache\_hit}(r, \text{route}(r))) $$
The challenge is that the cache state $C_i$ of worker $w_i$ changes dynamically due to LRU evictions, and the router's view $V$ of the cluster state must be kept consistent with minimal latency overhead.

\section{Proposed Solution}
We implement a two-part system:
\begin{enumerate}
    \item \textbf{Stateful Router}: A centralized service that maintains a mapping of `PrefixHash \rightarrow Set[WorkerID]`. It uses a "Sticky-Least-Loaded" policy.
    \item \textbf{Eviction-Aware Workers}: Modified vLLM engines that hook into the `BlockManager` to detect when a prefix block is freed and asynchronously report this to the router.
\end{enumerate}

\section{Algorithm}

\subsection{Prefix Hashing Strategy}
To avoid the "Whole Prompt Hashing" pitfall where variable user queries invalidate the cache key, we employ a \textbf{Fixed-Length Prefix Hashing} strategy.
$$ H(P) = \text{SHA256}(\text{Tokenize}(P)[:L]) $$
where $L$ is a configurable prefix length (e.g., system prompt length). This ensures that requests sharing the same system prompt map to the same hash, regardless of the subsequent user query.

\subsection{State Reconciliation (Anti-Entropy)}
To address the "Phantom Cache" problem (where the router believes a worker has a cache that was silently evicted or never created), we implement a \textbf{State Reconciliation Loop}.
\begin{itemize}
    \item \textbf{Push (Fast path)}: Immediate eviction reports via `/internal/eviction`.
    \item \textbf{Sync (Slow path)}: Every $T$ seconds (e.g., 5s), workers send a full list of active hashes via `/internal/sync`. The router replaces its view of the worker's state with this authoritative list, removing any stale entries.
\end{itemize}

\subsection{Routing Policy}
\begin{algorithm}
\caption{Sticky-Least-Loaded Routing with Prefix Awareness}
\begin{algorithmic}[1]
\Procedure{RouteRequest}{$req$}
    \State $L \gets req.prefix\_len \textbf{ if } req.prefix\_len \textbf{ else } \text{DefaultLen}$
    \State $h \gets \text{ComputeHash}(req.prompt, L)$
    \State $candidates \gets \text{GlobalMap}.get(h)$
    \If{$candidates \neq \emptyset$}
        \State \Comment{Cache Hit}
        \State \Return $\text{argmin}_{w \in candidates} (\text{Load}(w))$
    \Else
        \State \Comment{Cache Miss}
        \State $target \gets \text{argmin}_{w \in W} (\text{Load}(w))$
        \State $\text{GlobalMap}.update(h, target)$ \Comment{Speculative Update}
        \State \Return $target$
    \EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

\section{Design Rationale \& Architecture}

\subsection{The Disconnect: Compute vs. Routing}
The inception of this system stems from a critical observation: \textbf{There is a disconnect between the "Compute Layer" (vLLM) and the "Routing Layer" (Load Balancer).}
\begin{itemize}
    \item \textbf{Local Optimization}: vLLM excels at local optimization via PagedAttention and block reuse.
    \item \textbf{Global Blindness}: Standard load balancers (Round-Robin, Least-Connection) are stateless and unaware of the distributed cache state.
\end{itemize}
This leads to "Cache Blindness," where requests are routed to workers that must recompute the KV cache, wasting GPU cycles.

\subsection{Why "Push-Based" Consistency?}
Existing solutions typically use "Pull" (polling) or "Estimation" (heuristics).
\begin{itemize}
    \item \textbf{Pull}: High network overhead and latency.
    \item \textbf{Estimation}: Prone to "False Hits" if a worker silently evicts a block due to memory pressure.
\end{itemize}
Our solution employs a \textbf{Push-Based Eviction Callback} mechanism. The worker is the source of truth. When vLLM evicts a block, it immediately notifies the router. This ensures the router's map is \textit{eventually consistent} with minimal latency, effectively "bringing the compute to the data" rather than moving the data (as in LMCache).

\subsection{Value Proposition}
\begin{enumerate}
    \item \textbf{Reduced TTFT}: For RAG workloads with long contexts (e.g., 10k tokens), skipping prefill can reduce Time-To-First-Token from seconds to milliseconds.
    \item \textbf{Higher Throughput}: Reusing cache frees up GPU compute for decoding, increasing overall cluster RPS.
    \item \textbf{Cost Efficiency}: Maximizes ROI on GPU cycles by eliminating redundant work.
\end{enumerate}

\section{Production Readiness \& Roadmap}

\subsection{Current Status: Functional Prototype}
The system is a functional prototype demonstrating the core logic. However, for mission-critical deployment, several gaps must be addressed:
\begin{itemize}
    \item \textbf{Single Point of Failure}: The Router is currently a single process. Production would require multiple replicas with a shared state store (e.g., Redis).
    \item \textbf{State Persistence}: In-memory state is lost on restart.
    \item \textbf{Security}: Internal endpoints need authentication (mTLS).
\end{itemize}

\subsection{Roadmap}
\begin{enumerate}
    \item \textbf{Phase 1: Robustness}: Move `GlobalCacheMap` to Redis and implement consistent hashing for fallback routing.
    \item \textbf{Phase 2: Upstreaming}: Propose a standard "Callback Interface" RFC to the vLLM community to replace our current patch.
    \item \textbf{Phase 3: Advanced Features}: Support partial matching (Radix routing) and intelligent load shedding.
\end{enumerate}

\section{Implementation Details}

\subsection{Router Service}
Implemented in Python using \texttt{FastAPI}. The `GlobalCacheMap` is a thread-safe dictionary.
\begin{itemize}
    \item \textbf{Endpoint} `/v1/completions`: Proxies requests to the chosen worker.
    \item \textbf{Endpoint} `/internal/eviction`: Receives batched eviction reports.
\end{itemize}

\subsection{vLLM Modifications}
We inject a hook into `vllm.core.block_manager.BlockManager.free_block`.
\begin{lstlisting}[language=Python]
def free_block(self, block):
    if block.is_cached_prefix:
        eviction_queue.put(block.hash)
    # ... original freeing logic ...
\end{lstlisting}
A background thread (`EvictionReporter`) flushes this queue every 100ms to prevent network saturation.

\section{Research Level Assessment}
This project exhibits characteristics of a \textbf{Graduate-Level Systems Research Project}.
\begin{itemize}
    \item \textbf{Novelty}: While prefix caching is known, the specific mechanism of \textit{push-based eviction callbacks} to maintain a lightweight global state without distributed storage is a distinct design point in the trade-off space (vs. LMCache's remote storage or SGLang's radix tree sharing).
    \item \textbf{Complexity}: It requires modifying the internal memory management of a production-grade system (vLLM) and building a distributed coordination layer.
    \item \textbf{Evaluation}: A proper research paper would require evaluating TTFT reduction and "Stale Cache Rate" (requests routed to a worker that just evicted the data) against baselines like:
    \begin{itemize}
        \item \textbf{Round-Robin}: The standard stateless baseline.
        \item \textbf{Consistent Hashing}: A standard distributed systems technique (e.g., Chord ring) that provides locality but lacks awareness of dynamic evictions.
        \item \textbf{Splitwise}: A state-of-the-art inference serving system that separates prefill and decode phases, offering a strong baseline for throughput comparison.
    \end{itemize}
\end{itemize}

\section{Conclusion}
We have designed and implemented a prototype for stateful routing that addresses the "cache blindness" of standard load balancers. By coupling routing decisions with real-time cache lifecycle events (eviction), we can significantly improve cluster efficiency for RAG and multi-turn conversation workloads.

\end{document}
