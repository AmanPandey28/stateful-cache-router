# Quick Reference Card - Submission

## üéØ Key Points for Presentation

### Problem
- RAG systems have shared prefixes (documents, system prompts)
- Standard load balancers are cache-blind
- Redundant prefill computation wastes GPU cycles

### Solution
- **Block-based KV cache** (16 tokens/block, 924 blocks per GPU)
- **Prefix-aware routing** (longest prefix match)
- **Reference counting** (shared blocks, no duplicates)
- **Priority queue eviction** (oldest first, tie-break by sequence)

### Implementation Highlights

1. **Model**: Llama 2 13B on A100 40GB
   - 40 layers, 40 heads, 128 dim, FP16
   - 924 blocks capacity

2. **Block System**: 16 tokens per block
   - SHA-256 hashing (consistent)
   - Only full blocks cached
   - 13,107,200 bytes/block

3. **Routing**: Longest prefix match
   - Router maintains prefix tree
   - Routes to worker with most cached blocks
   - Falls back to least-loaded on miss

4. **Latency**: Realistic simulation
   - Prefill: `5ms + (blocks_to_compute * 2.5ms)`
   - Decode: `tokens * 15ms`
   - Accounts for cached blocks

### Results

- **Cache Hit Rate**: 35-50% (RAG workloads)
- **Latency Improvement**: HIT significantly faster than MISS
- **Load Balancing**: Effective across workers
- **Prefix Matching**: Tracks match quality

### Files to Show

1. **Core Implementation**:
   - `router/main.py` - Routing logic
   - `router/cache_map.py` - Prefix tree
   - `scripts/mock_worker.py` - Worker emulator

2. **Results**:
   - `dashboard_results.json` - All metrics
   - `*.png` - Visualizations

3. **Documentation**:
   - `README.md` - Overview
   - `SUBMISSION_GUIDE.md` - Complete guide

## üöÄ Quick Demo Commands

```bash
# Terminal 1: Router
python -m router.main

# Terminal 2: Workers
python scripts/start_multiple_workers.py

# Terminal 3: Dashboard
python scripts/results_dashboard.py
```

## üìä Key Metrics to Mention

1. **Cache Hit Rate**: Shows effectiveness of prefix matching
2. **Latency (HIT vs MISS)**: Demonstrates performance improvement
3. **Worker Distribution**: Shows load balancing
4. **Match Length**: Shows prefix match quality

## üí° Talking Points

1. **Block-Based System**: "We use 16-token blocks, matching vLLM's PagedAttention"
2. **Prefix Matching**: "Router finds worker with longest matching prefix"
3. **Reference Counting**: "Shared blocks counted, preventing duplicates"
4. **Realistic Latency**: "Only computes blocks not in cache"
5. **Baseline Comparison**: "Framework ready for round-robin and least-loaded"

## ‚ö†Ô∏è If Asked About Limitations

1. **Emulator**: "Current implementation is an emulator for research; real vLLM integration is future work"
2. **Baselines**: "Baseline comparison framework is ready; requires router restart with different strategy"
3. **Prefix Length**: "Currently fixed at 16 tokens (one block); can be tuned"

## ‚úÖ Submission Checklist

- [x] All requirements implemented
- [x] Results collected
- [x] Visualizations generated
- [x] Documentation complete
- [x] Code verified

**Status: READY FOR SUBMISSION** ‚úÖ

