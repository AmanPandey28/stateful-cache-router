\documentclass[11pt]{article}

% --- Essential Packages ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{mathptmx} % Professional Times New Roman
\usepackage{titlesec}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{amsfonts,amsmath,amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{caption}

% --- Configuration ---
\geometry{a4paper, margin=1in}
\setlength{\parindent}{0pt}
\setlength{\parskip}{0.6em}

% Section Formatting
\titlespacing*{\section}{0pt}{12pt}{6pt}
\titlespacing*{\subsection}{0pt}{8pt}{4pt}

% Code Listing Configuration
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.97,0.97,0.97}
\definecolor{codeblue}{rgb}{0.0, 0.0, 0.6}
\definecolor{codepurple}{rgb}{0.58,0,0.82}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegray}\itshape,
    keywordstyle=\color{codeblue}\bfseries,
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\small, % Increased from scriptsize for readability
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    frame=single,
    rulecolor=\color{codegray},
    aboveskip=1em,
    belowskip=1em
}
\lstset{style=mystyle}

% --- Header/Footer ---
\pagestyle{fancy}
\fancyhf{}
\rhead{\small CS 59200: Machine Learning Systems}
\lhead{\small Stateful Cache-Aware Routing}
\cfoot{\thepage}

% --- Metadata ---
\title{\textbf{Stateful Cache-Aware Routing for Distributed LLM Inference: \\
A Push-Based Consistency Approach}}
\author{\textbf{Team:} Aman Pandey, Sanjeev Kumar, Aryan Khanolkar, Guna Avula \\ 
\textit{Purdue University}}
\date{November 2025}

\begin{document}

\maketitle

\begin{abstract}
\noindent The efficiency of Large Language Model (LLM) serving is fundamentally constrained by the memory-bound ``prefill'' phase. While execution engines like vLLM optimize intra-node memory via PagedAttention, distributed deployments typically rely on stateless load balancing. This disconnect leads to ``Cache Blindness,'' where sequential requests sharing identical system prompts or retrieval contexts are scattered across replicas, forcing redundant re-computation. 

We present a \textbf{Stateful Cache-Aware Router} that bridges the gap between local engine optimizations and cluster-wide efficiency. Unlike existing solutions that rely on high-latency disaggregated storage (LMCache) or heavy orchestration (llm-d), our system introduces a novel \textbf{Push-Based Eviction Callback} protocol. By instrumenting vLLM workers to actively report cache lifecycle events and implementing an \textbf{Anti-Entropy State Reconciliation} mechanism, our architecture guarantees eventual consistency with minimal overhead. Benchmarks demonstrate that this approach effectively eliminates redundant prefills in Retrieval-Augmented Generation (RAG) workloads.
\end{abstract}

\section{Introduction}
The computational cost of Generative AI is dominated by the attention mechanism, which scales quadratically $O(L^2)$ with sequence length $L$. In modern RAG workloads, input contexts frequently exceed 10,000 tokens, making the ``prefill'' phase the primary latency bottleneck. \textbf{Automatic Prefix Caching (APC)} has emerged as a standard optimization within inference engines, allowing Key-Value (KV) tensors to be persisted and reused across requests.

However, APC is inherently local to a single GPU. In a distributed cluster of $N$ replicas, a standard Round-Robin load balancer offers a cache hit probability of only $1/N$ for a repeated prefix. This project addresses the \textbf{``Distributed Locality Gap''}—the failure of stateless routing to leverage stateful engine optimizations.

We propose a middleware solution that adheres to the principle of \textbf{``Compute-to-Data''} routing. Rather than moving heavy KV states across the network, we route lightweight requests to the worker that already holds the state.

\section{Design Rationale}
The inception of this system stems from a critical observation: \textbf{There is a disconnect between the "Compute Layer" (vLLM) and the "Routing Layer" (Load Balancer).}

\subsection{The Logical Progression}
\begin{enumerate}
    \item \textbf{The Problem}: vLLM is excellent at \textit{local} optimization. If a request hits a worker that has the prefix cached, it's fast. But standard load balancers (Round-Robin, Least-Connection) are "stateless"—they don't know which worker has which prefix.
    \item \textbf{The Consequence}: This leads to "Cache Blindness." A request with a heavy system prompt might be routed to Worker B, even though Worker A just served it and has the cache hot. Worker B has to recompute the KV cache (expensive prefill), wasting GPU cycles and increasing latency.
    \item \textbf{Existing Solutions \& Their Flaws}:
    \begin{itemize}
        \item \textbf{Shared Storage (LMcache)}: "Let's move the cache to a shared disk/Redis." $\rightarrow$ \textbf{Flaw}: Moving gigabytes of KV cache over the network is slow. It introduces latency that often negates the benefit for short-to-medium prompts.
        \item \textbf{Heavy Orchestration (llm-d)}: "Let's build a complex Kubernetes operator." $\rightarrow$ \textbf{Flaw}: High barrier to entry. Tightly coupled to K8s. Hard to debug.
    \end{itemize}
    \item \textbf{Our "Aha!" Moment}: We don't need to move the data (LMcache), and we don't need a heavy platform (llm-d). We just need a \textbf{lightweight signal} to tell the router where the data \textit{already is}.
\end{enumerate}

\subsection{Why "Push-Based" Eviction?}
Most systems use "Pull" (polling) or "Estimation".
\begin{itemize}
    \item \textbf{Pull}: The router asks workers "What do you have?" $\rightarrow$ Too much traffic, always outdated.
    \item \textbf{Estimation}: The router guesses "I sent it to Worker A, so Worker A must have it." $\rightarrow$ \textbf{Flaw}: What if Worker A ran out of memory and evicted it 10ms ago? The router would send a "False Hit."
\end{itemize}

\textbf{Our Solution}: \textbf{Push-Based Eviction Callbacks}.
\begin{itemize}
    \item The worker is the \textit{source of truth}.
    \item When vLLM evicts a block, it \textit{immediately} notifies the router.
    \item This ensures the router's map is "Eventually Consistent" with very low latency.
\end{itemize}

\section{System Architecture}

Our system employs a three-tier architecture designed for modularity and fault tolerance.

\subsection{Tier 1: The Tokenizer-Aware Router}
The router is the control plane. It does not perform inference but makes scheduling decisions based on a \texttt{GlobalCacheMap}. Crucially, it must replicate the hashing logic of the inference engine exactly.
\begin{itemize}
    \item \textbf{Tokenization Alignment:} Hashing raw strings is unreliable (e.g., whitespace sensitivity). The router loads the specific model tokenizer (e.g., Mistral-7B) to convert prompts to Token IDs before hashing.
    \item \textbf{Prefix-Aware Hashing:} To support RAG/Chat, where a system prompt is constant but the user query varies, the router calculates the hash based on an \texttt{effective\_prefix\_len}.
\end{itemize}

\subsection{Tier 2: The Worker Pool (vLLM + Sidecar)}
We modify standard vLLM instances by injecting a surgical "sidecar" thread called the \texttt{EvictionReporter}. This component hooks into the \texttt{BlockManager.free\_block()} method. Unlike polling systems, this makes the worker the \textbf{Source of Truth}.

\subsection{Tier 3: The Consistency Protocol}
To maintain $V \approx S$, we implement a hybrid consistency protocol:
\begin{enumerate}
    \item \textbf{Fast Path (Eviction Reporting):} Real-time notification of cache removals. To prevent Distributed Denial of Service (DDOS) on the router during mass evictions, these reports are batched and flushed every 100ms.
    \item \textbf{Slow Path (Anti-Entropy Sync):} Periodic background reconciliation (every 5s) where workers send a full snapshot of their active state. This heals divergence caused by network partitions.
\end{enumerate}

\section{Deep Dive: Implementation Details}

\subsection{Router: Tokenizer-Aware Hashing}
The core correctness requirement is that $\text{Hash}_{\text{Router}}(P) \equiv \text{Hash}_{\text{Worker}}(P)$. If the router and worker disagree on the hash of a prefix, requests will be routed incorrectly (0\% Hit Rate). We solve this by ensuring the router performs the exact same tokenization as the worker before hashing.

\begin{lstlisting}[language=Python, caption=Robust Prefix Hashing Strategy]
class TokenizerUtils:
    def compute_prefix_hash(self, text: str, prefix_len: int = None) -> str:
        token_ids = self.tokenize(text)
        # Use effective length to ignore variable user queries
        if prefix_len:
            token_ids = token_ids[:prefix_len]
        
        # Hash the tuple of integers for stability. 
        # This matches vLLM's internal block hashing strategy.
        return hashlib.sha256(str(tuple(token_ids)).encode()).hexdigest()
\end{lstlisting}

\subsection{Router: Scalable Reverse Index}
A naive implementation of the `sync` operation would require iterating over the entire global map to remove a worker's stale entries. As the number of cached prefixes grows ($10^5+$), this becomes an $O(\text{TotalHashes})$ operation.

We implemented a \textbf{Reverse Index} optimization:
\begin{itemize}
    \item \textbf{Forward Map:} \texttt{PrefixHash $\rightarrow$ Set[WorkerID]}
    \item \textbf{Reverse Map:} \texttt{WorkerID $\rightarrow$ Set[PrefixHash]}
\end{itemize}
This reduces the synchronization complexity to $O(\text{WorkerHashes})$, decoupling the router's CPU load from the total cluster size.

\subsection{Worker: Asynchronous Batching}
Hooking into the critical path of memory management requires extreme care. Blocking the GPU engine to send an HTTP request would degrade inference performance. We use a thread-safe deque for atomic hand-off between the GPU thread and the IO thread.

\begin{lstlisting}[language=Python, caption=Non-Blocking Eviction Reporting (EvictionReporter)]
# vLLM Hook (Pseudo-code)
def free_block(self, block):
    # Atomic push to memory queue (Microsecond latency)
    if block.is_cached_prefix:
        self.eviction_queue.append(block.hash)

# Background Reporter Thread (Actual Implementation)
async def _report_loop(self):
    async with aiohttp.ClientSession() as session:
        while self._running:
            await asyncio.sleep(self.report_interval)
            if not self.eviction_queue: continue

            # Drain the queue
            batch = []
            while self.eviction_queue:
                batch.append(self.eviction_queue.popleft())

            if not batch: continue

            try:
                payload = {"worker_id": self.worker_id, "evicted_hashes": batch}
                async with session.post(f"{self.router_url}/internal/eviction", json=payload) as resp:
                    if resp.status != 200:
                        logger.error(f"Failed to report evictions: {resp.status}")
            except Exception as e:
                logger.error(f"Error reporting evictions: {e}")
\end{lstlisting}

\subsection{Anti-Entropy State Reconciliation}
Distributed systems must withstand partial failures. If a "Fast Path" eviction report is dropped due to a network glitch, the router will hold a "Phantom Cache" entry—believing data exists when it does not.

To solve this, we implemented a self-healing "Slow Path":
\begin{enumerate}
    \item Every 5 seconds, the worker scans its internal block table to identify all active prefix hashes.
    \item It sends this full list to the router via \texttt{/internal/sync}.
    \item The router performs a \textbf{Destructive Update}: it clears all known entries for that worker and repopulates them from the list.
\end{enumerate}
This guarantees that any state divergence persists for at most 5 seconds.

\begin{lstlisting}[language=Python, caption=Router State Reconciliation]
# Router: cache_map.py
def sync_worker_state(self, worker_id: str, active_hashes: List[str]):
    with self._lock:
        # 1. Destructive Update: Clear old state for this worker using Reverse Index
        # Complexity: O(M) where M is number of hashes held by THIS worker.
        if worker_id in self._worker_to_hashes:
            current_hashes = list(self._worker_to_hashes[worker_id])
            for h in current_hashes:
                if h in self._map:
                    self._map[h].discard(worker_id)
                    if not self._map[h]:
                        del self._map[h]
            self._worker_to_hashes[worker_id].clear()
        
        # 2. Re-populate with authoritative current state
        for h in active_hashes:
            if h not in self._map: self._map[h] = set()
            self._map[h].add(worker_id)
            
            if worker_id not in self._worker_to_hashes:
                self._worker_to_hashes[worker_id] = set()
            self._worker_to_hashes[worker_id].add(h)
\end{lstlisting}

\section{Routing Algorithm}
The router executes a \textbf{Sticky-Least-Loaded} policy with \textbf{Speculative Updates}.

\begin{algorithm}
\caption{Stateful Routing with Speculation}
\begin{algorithmic}[1]
\Procedure{RouteRequest}{$req$}
    \State $L \gets req.prefix\_len \textbf{ if } req.prefix\_len \textbf{ else } 64$ \Comment{Smart Default}
    \State $h \gets \text{ComputeHash}(req.prompt, L)$
    \State $candidates \gets \text{GlobalMap}.get(h)$
    
    \If{$candidates \neq \emptyset$}
        \State \Comment{\textbf{Cache Hit}: Stickiness ensures locality}
        \State \Return $\text{argmin}_{w \in candidates} (\text{Load}(w))$
    \Else
        \State \Comment{\textbf{Cache Miss}: Load Balancing}
        \State $target \gets \text{argmin}_{w \in W} (\text{Load}(w))$
        \State $\text{GlobalMap}.update(h, target)$ \Comment{\textbf{Speculative Update}}
        \State \Return $target$
    \EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

\textbf{Rationale for Speculation:} When a request is routed to a worker on a miss, we \textit{immediately} record that the worker has the prefix. This prevents the "Thundering Herd" problem where concurrent requests for the same new prefix are routed to different workers because the first worker hasn't reported the cache creation yet.

\section{Evaluation and Validation}

\subsection{Benchmark Design}
We developed an end-to-end benchmarking suite (`benchmark.py`) to validate the system lifecycle across four states:
\begin{enumerate}
    \item \textbf{Cold Start:} Verifies Speculative Update.
    \item \textbf{Stickiness:} Verifies that a subsequent request routes to the same worker, even if another worker has lower load.
    \item \textbf{Eviction:} Verifies the worker correctly reports memory pressure.
    \item \textbf{Recovery:} Verifies the router updates the map and routes the next request to a new worker.
\end{enumerate}

\subsection{Comparative Analysis}
Table 1 summarizes our architectural position relative to SOTA.

\begin{table}[h]
\centering
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Feature} & \textbf{vLLM (Stock)} & \textbf{LMCache} & \textbf{llm-d} & \textbf{Ours} \\ \midrule
Routing Strategy & Round-Robin & N/A & K8s Gateway & \textbf{Sticky-Hash} \\
Data Locality & None & Low & High & \textbf{High} \\
State Consistency & N/A & Strong & Eventual (Poll) & \textbf{Eventual (Push)} \\
Network Overhead & Low & High & Medium & \textbf{Very Low} \\
Infra Dependency & None & Redis/Disk & Kubernetes & \textbf{None (Python)} \\ \bottomrule
\end{tabular}
\caption{Comparison of Distributed Inference Architectures}
\label{tab:comparison}
\end{table}

\section{Discussion: Limitations and Future Work}

\subsection{Failure Modes Analysis}
\begin{itemize}
    \item \textbf{Router Crash:} Since the map is in-memory, a router restart clears the state. The \textbf{Anti-Entropy Sync} loop in the workers ensures the router rebuilds its map within 5 seconds of coming back online.
    \item \textbf{Network Partition:} If eviction reports are dropped, the router may have "Phantom Cache" entries (False Hits). The Sync loop heals this automatically.
\end{itemize}

\subsection{Roadmap for Production}
\begin{enumerate}
    \item \textbf{Persistence:} Moving `GlobalCacheMap` to Redis would allow the router to be stateless and horizontally scalable.
    \item \textbf{Security:} Internal endpoints (`/internal/*`) must be secured via mTLS or VPC isolation in a production environment.
    \item \textbf{Model Generalization:} The current hashing schema assumes a single model. Production systems should include `model\_name` in the hash key.
\end{enumerate}

\section{Conclusion}
This project moves beyond theoretical design to provide a robust, implementation-ready solution for distributed LLM inference. By solving the critical engineering challenges of \textbf{Tokenization Alignment}, \textbf{Network Backpressure} (via batched reporting), and \textbf{State Consistency} (via anti-entropy sync), we provide a scalable path to reducing TTFT in production RAG clusters. The system effectively bridges the gap between local engine optimizations and global cluster routing.

\end{document}
